# 监督学习

# 梯度下降：

## 简单原理（以一元为例）：

1. 确定预测函数（线性回归为例） ---直线....

   一元线性回归：寻找斜率参数w 确定预测函数y=wx

   <img src="C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220517154935493.png" alt="预测函数" style="zoom:33%;" /> 

   

2. 确定代价函数

   量化数据偏离程度（误差）--常见：（均方误差）确定参数

   ----最小二乘法 ---得到二次函数

   若代价函数为**y=wx+b** （两个参数）------>**抛物面**  最低点

   **最小二乘法**也可以**拟合高阶曲线**

   最小二乘法就是概率密度是高斯分布时最大自然估计的特殊情况

   

   <img src="C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220517160005605.png" alt="代价函数" style="zoom:25%;" /> 

3. 明确搜索方向--**梯度**计算（代价函数导数--斜率）

   <img src="C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220517154609086.png" alt="学习率" style="zoom:33%;" /> 

4. **学习率**（决定步长）

   学习率*斜率=步长 （越接近驻点越慢-----最终收敛在最低点）

5. 循环3.4直到找到最低点 ---确定参数w

## 算法：

​	1.bgd：批量梯度下降法（稳）

<img src="C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220517155053821.png" alt="image-20220517155053821" style="zoom:25%;" /> 

​	2.sgd随机梯度下降法（快，缺乏精准度）		

<img src="C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220517155119867.png" alt="image-20220517155119867" style="zoom:25%;" />                      

3. mbgd 小批量梯度下降法(又快又稳)

   <img src="C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220517155252259.png" alt="image-20220517155252259" style="zoom:25%;" />     

4. 瞎子下山：最速下降法

## 优缺点：

1. 对学习率敏感：

   学习率太大：反复横跳  

   太小：效率低，浪费计算量

2. 处理bgd外 无法保证找到全局最优解



## 更优的下降算法：

1. AdaGrad  --动态学习率 
2. RMSProp--优化动态学习率
3. AdaDelta --无需设置学习率
4. Adam  --融合AdaGrad 和RMSProp
5. Momentun ---模拟动量（惯性）
6. FTRL  
7. ...

# 反向传播：

随机参数---->损失函数 误差*权重----->最小二乘法/梯度下降-->更新参数

**动力学角度**：

**非线性动力学系统**：

- 混沌理论
- 三体问题
- 神经网络
- 生态、免疫系统，经济学、量子场理论

蝴蝶效应：神经网络  参数的微小变化通过非线性 空间映射会发生巨大差异

损失函数、学习率、梯度下降算法

# 神经网络基本模型

（对数据有效决策分类）

<img src="C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220517162112252.png" alt="image-20220517162112252" style="zoom:25%;" /> 

## 单层神经网络：输入 输出

### 神经元模型（感知机）

<img src="C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220517163419993.png" alt="image-20220517163419993" style="zoom: 67%;" /> 

#### 激活函数（线性叠加---->加入非线性）：

 <img src="C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220517163447769.png" alt="image-20220517163447769" style="zoom: 50%;" />

#### 效果：

　我们可以用**决策分界**来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，当数据的维度是3维的时候，就是划出一个平面，当数据的维度是n维时，就是划出一个n-1维的超平面。

 

## 两层：

###	特点

两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，**中间层和输出层都是计算层**

（可能存在**偏置结点**：无输入，仅储存）

### 功能：

#### 		1.线性分类（空间变换）

　与单层神经网络不同。**理论证明，两层神经网络可以无限逼近任意连续函数。**

也就是说，面对复杂的非线性分类任务，两层（带一个隐藏层）神经网络可以分类的很好。

#### 	原因：

隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。

两层神经网络可以做非线性分类的关键--隐藏层。联想到我们一开始推导出的矩阵公式，我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。因此，**隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。**

　　两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，多层的神经网络的本质就是复杂函数拟合。

#### 2.BP算法（反向传播）实现训练、预测

一般利用**sigmoid**函数作激活函数（达到阈值响应）

最小化损失函数（**代价函数**）-------通过**梯度下降**和**神经网络反向传播**确定权重



<!--ps:　反向传播算法的启示是数学中的链式法则。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从BP算法开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。机器学习问题之所以称为学习问题，而不是优化问题，就是因为它不仅要求数据在训练集上求得一个较小的误差，在测试集上也要表现好。因为模型最终是要部署到没有见过训练数据的真实场景。提升模型在测试集上的预测效果的主题叫做**泛化** （generalization），相关方法被称作正则化（regularization）。神经网络中常用的泛化技术有权重衰减等          90年代中期，由Vapnik等人发明的SVM（Support Vector Machines，支持向量机）算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM迅速打败了神经网络算法成为主流。-->

## 多层：（深度学习）

### 特点：

更高维度，更深入的**表示特征**，以及更强的函数模拟能力。

更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的**抽象表示更深入。**例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。

<img src="C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220517171012917.png" alt="image-20220517171012917" style="zoom:25%;" /> 

#### 训练与预测：

**激活函数**：*ReLU函数*

ReLU函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是ReLU函数。ReLU函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是y=max(x,0)。简而言之，在x大于0，输出就是输入，而在x小于0时，输出就保持为0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。

<!--在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现**过拟合现象**。因此正则化技术就显得十分重要。目前，Dropout技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术。-->

### 功能：

更好的区分与分类能力,以及更强的函数模拟能力。　更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的**容量**（capcity）去拟合真正的关系。

## 总结：

1.对于高维数据 ：分类线----->分类面

2.概念 泛化（到测试集）  过拟合：模型过于复杂不能很好泛化到测试集  欠拟合：过于简单的模型

维，提高线性转换能力 ，通过多个隐藏层，增加非线性转换次数。

5. 其他模型：卷积神经网络（CNN）  循环神经网络...

    

# 卷积

## 线性卷积

### 连续卷积：

### 理解

越早的输入，贡献越小，所以这就是说，某一时刻的输出是之前很多次输入乘以各自的衰减系数之后的叠加而形成某一点的输出，然后再把不同时刻的输出点放在一起，形成一个函数，这就是卷积，卷积之后的函数就是输出信号大小随时间变化的函数。

![image-20220805155114921](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805155114921.png)

可以证明，关于几乎所有的实数*x*，上述积分是存在的。这样，随着*x*的不同取值，这个积分就定义了一个新函数*h(x)*，称为函数*f*与*g*的卷积，记为*h(x)=(f\*g)(x)*

对于二维甚至多维空间的积分卷积，定义同理。

## 离散卷积：

![image-20220805155128029](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805155128029.png)

实际上，离散卷积和积分卷积的本质是一样的，可以轻松互相转化。

## 卷积性质：

![image-20220805155240513](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805155240513.png)

## 周期卷积

![image-20220805155513042](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805155513042.png)

## 循环卷积：

对于定义域是有限区间的函数，循环卷积就是函数做周期延展之后的周期卷积。

## 高斯卷积

## 滤波器

## 卷积定律：

函数卷积的傅里叶变换 = 函数傅里叶变换的乘积

![image-20220805160318671](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805160318671.png)

## 应用：

一个系统有不稳定的输入，稳定的输出，求系统的存储量就是卷积

### 通信：

将一种信号搬移到另一种频率中，实现调制功能

### 物理：

系统对某输入物理量影响或污染

### 电路

某系统冲击函数对输入的响应

线性时不变系统。

### 信号处理：

滤波器

### 机器学习 ：cnn

计算机视觉

对图像卷积

卷积核 --二维离散卷积

<img src="C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805161441288.png" alt="image-20220805161441288" style="zoom:50%;" />

对应相乘相加

![image-20220805161758718](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805161758718.png)

![image-20220805162015976](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805162015976.png)

![image-20220805162043765](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805162043765.png)

# 卷积神经网络：cnn





![image-20220805162536141](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805162536141.png)

**全连接网络**：**图像**而言，参数众多难以训练，容易过拟合。

**卷积运算**：简洁

## 卷积层

### 卷积核： 空间平移不变

![image-20220805162852348](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805162852348.png)



如图感受野：3

![image-20220805162913547](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805162913547.png)

多层卷积：深度卷积神经网络：

![image-20220805163012646](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805163012646.png)

### 非线性：激活函数：relu 函数  线性整流单元

![image-20220805163200768](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805163200768.png)

只把负数变0

## 池化层：pooling

放大主要特征，减少偏差

- 最大池化
- 平均池化

![image-20220805163413740](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805163413740.png)



## 全连接层

![image-20220805163500817](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805163500817.png)

# 循环神经网络RNN

对时序数据建模

隐层神经元在时间维度的关联（重复）

![image-20220805164126389](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805164126389.png)



## 用途：

语言类： 语序、时间相关

## 图解

![image-20220805164304271](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805164304271.png)

![image-20220805164320778](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805164320778.png)

![image-20220805164332395](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220805164332395.png)

## 特性：

- 时序建模
- 记忆能力

- 自然语言处理领域 nlp

- 训练也和传统神经网络一样采用误差反向传播， 但隐藏层计算要引入不同时刻数据

- 记忆短期（10步    ----》长短期记忆网络lstm
- 更灵活   io：1 to n  ； n to 1 ; n to n  ; n to m(机器翻译)

​	（注意力机制 的transformer模型

# LSTM-RNN

**长短期记忆网络**

rnn----短期

- 增加新的时间链ct 记录 long-term memory

![image-20220818084258889](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818084258889.png)

![image-20220818084331448](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818084331448.png)

![image-20220818084343186](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818084343186.png)

## 训练：

比rnn复杂，**梯度下降**，模拟大脑关注关键信息，忽略无关信息

## 应用：

Google翻译，对话机器人，语音合成

与cnn，反向传播构成三大基石

# attention机制

**attention==权重**

## 核心思想：

通过加权求和解决context(上下文的理解，再在不同上下文中关注不同的信息

## encoder-decocer-rnn

![image-20220818085813517](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818085813517.png)

## 引入attention

![image-20220818085850830](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818085850830.png)

**神经网络训练参数**

![image-20220818085922195](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818085922195.png)

![image-20220818085938378](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818085938378.png)

## self-attention机制

简化了rnn

翻译后一个单词games选择对应beijing，winter都权重高的attention score

![image-20220818090107252](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818090107252.png)

## 模型

![image-20220818090327709](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818090327709.png)

**很好的嵌入上下文信息**

重复

![image-20220818090404323](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818090404323.png)

## 应用

- transformer
- bert
- gnn

# transformer模型

nlp

![image-20220818093059286](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818093059286.png)



self-attention：拆解对照表，根据权重标明关系，嵌入上下文信息



前馈网络：根据权重变形一次

## 编码解码

- encoder实现了对语言语法和上下文理解，相当于语言拆解对照表
- decoder 实现了一种语言到另一种的映射，相当于语言组装对照表。



![image-20220818093354150](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818093354150.png)



![image-20220818093819250](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818093819250.png)

## 矩阵计算：

### 过程

![image-20220818093946845](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818093946845.png)

![image-20220818093953447](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818093953447.png)



![image-20220818094022931](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818094022931.png)

### 即：通过矩阵运算实现权重计算

简化：

<img src="C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818094128964.png" alt="image-20220818094128964" style="zoom:33%;" />

![image-20220818094141092](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818094141092.png)



不同权重矩阵进行8次计算：

目的：消除qkv初始值影响。

![image-20220818094246000](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818094246000.png)



总结：

![image-20220818093115157](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818093115157.png)



![image-20220818094436862](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818094436862.png)

### 输出：

**向量--->单词**

![image-20220818093310181](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818093310181.png)

概率最高的单词就是最后的输出

## 训练：

梯度下降

# GNN：图神经网络简述

**非欧空间**

![image-20220818094955336](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818094955336.png)



## 1.邻接矩阵（表



## 2.聚合：

层内和层间结点间的消息传递

层内聚合：池化

![image-20220818095128478](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818095128478.png)

层间

![image-20220818095136445](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818095136445.png)



## **gcn**：图卷积：

**在层间通过领域聚合 实现卷积特征提取**

![image-20220818095156497](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818095156497.png)

![image-20220818095204360](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818095204360.png)

### 频域空间： 基于谱的图卷积网络



## gragh sage算法

先采样再聚合

## 图注意网络 gat

考虑了领域结点的权重，attention机制

## 图生成网络

![image-20220818102405527](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818102405527.png)

## 图时空网络

![image-20220818102413816](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818102413816.png)

# 变分自编码器VAE

**分类**

## 自编码器AE：

![image-20220818102635773](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818102635773.png)

**隐藏层少**：过滤输入，特征提取，减少变量，压缩



### 应用：取马赛克、水印、ai抠图



## 变分~：

将输入映射到一个分布上

![image-20220818102900308](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818102900308.png)

![image-20220818102943786](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818102943786.png)



### **损失函数**

![image-20220818103002663](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818103002663.png)

## pytorch代码：

![image-20220818103057304](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818103057304.png)

## 应用

- 图像压缩
- 图像降噪
- 图像分割：（自动驾驶
- 环境表示：deepmind
- 与强化学习结合

# BERT模型

**将transformer的encoder和decoder拆分**：

![image-20220818103824106](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818103824106.png)



nlp领域

![image-20220818103934509](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818103934509.png)

## 过程：

![image-20220818104003446](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818104003446.png)

### 预训练：

![image-20220818104415609](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818104415609.png)



![image-20220818104952483](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818104952483.png)

### 精调

![image-20220818104915382](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818104915382.png)



![image-20220818105011023](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818105011023.png)

## 总结：

3.4亿参数

将简单结构encoder的团结协作，相互交流

# svm支持向量机

**低维空间的混乱 高维空间的秩序**

## 分类算法：

![image-20220818105331125](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818105331125.png)

**svm硬分类：找到是margin（隔离带最大化的支持向量。**

**svm软分类：数据非线性但依然可分，总体最佳效果**

![image-20220818105543378](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818105543378.png)

### 非线性且不可分的问题

- 异或问题、嵌套样本

**svm通过kernel运算**，将数据**映射到高维空间**，实现分类

![image-20220818110450181](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818110450181.png)



## 空间



![image-20220818110546413](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818110546413.png)

低维到高维映射

![image-20220818110558982](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818110558982.png)

### svm核函数

![image-20220818110639001](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818110639001.png)

# hmm隐马尔可夫模型

状态转移

![image-20220818110833834](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818110833834.png)

马尔可夫：每个状态只和前一个时刻有关。

![image-20220818195905469](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818195905469.png)

前向算法。

![image-20220818195948147](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818195948147.png)



![image-20220818200017515](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818200017515.png)

前向后向算法：

![image-20220818200041992](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818200041992.png)



![image-20220818200059865](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818200059865.png)

viterbi算法（dp 最短路

![image-20220818200130696](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818200130696.png)

- 应用;语音识别
- ![image-20220818200312610](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818200312610.png)

# GAN生成对抗网络

![image-20220818200433770](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818200433770.png)

## 损失函数

![image-20220818200547667](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818200547667.png)

训练过程

![image-20220818200647868](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818200647868.png)

训练判别器=>收敛=》训练生成器---》收敛——》loop

![image-20220818200749807](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818200749807.png)



# 贝叶斯公式：

主客观世界桥梁

p(s)  : 对状态的先验假设     主观

p(o):观测分布

p(o|s):likelihood 可能性

p(s|o): 后验    

![image-20220818201521955](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818201521955.png)



加入主观认知

![image-20220818201544553](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818201544553.png)

应用

![image-20220818201641162](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818201641162.png)

生物：大脑

![image-20220818201652555](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818201652555.png)





# ViT :vision transformer

nlp cv



![image-20220818202746291](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818202746291.png)

**与bert区别**：输入将图向量化，转为embeddings的词结构，实现nlp中类似句子的效果。 

![image-20220818202941329](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818202941329.png)

## 步骤

![image-20220818203045221](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818203045221.png)



transformer与bert相同

![image-20220818203137720](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818203137720.png)



![image-20220818203203715](C:\Users\86172\AppData\Roaming\Typora\typora-user-images\image-20220818203203715.png)
